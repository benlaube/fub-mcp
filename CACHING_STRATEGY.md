# Caching Strategy for Bulk Queries

## Overview

Yes! **The server now caches data for bulk queries** to significantly improve performance and reduce API calls to Follow Up Boss.

## Benefits

‚úÖ **Faster responses** - Cached data returns in milliseconds vs seconds  
‚úÖ **Reduced API calls** - Saves on rate limits and API quota  
‚úÖ **Better user experience** - Repeated queries are instant  
‚úÖ **Cost savings** - Fewer API requests means lower API usage  

## How It Works

### Automatic Caching

1. **GET requests are cached** - All successful GET responses are stored
2. **Cache key generation** - Based on endpoint + parameters (MD5 hash)
3. **TTL (Time-To-Live)** - Different TTLs for different data types
4. **LRU eviction** - Least Recently Used entries are removed when cache is full

### Cache Invalidation

The cache is automatically invalidated when data changes:

- ‚úÖ **Creating contacts** ‚Üí Invalidates `/people` cache
- ‚úÖ **Updating contacts** ‚Üí Invalidates `/people` cache  
- ‚úÖ **Deleting contacts** ‚Üí Invalidates `/people` cache
- ‚úÖ **Creating custom fields** ‚Üí Invalidates `/customFields` cache
- ‚úÖ **Updating custom fields** ‚Üí Invalidates `/customFields` cache
- ‚úÖ **Deleting custom fields** ‚Üí Invalidates `/customFields` cache

## TTL Configuration by Data Type

Different types of data have different cache durations based on how frequently they change:

### Long-Lived Data (5 minutes TTL)
- `/customFields` - Custom fields rarely change
- `/pipelines` - Pipelines are stable
- `/stages` - Stages are stable
- `/users` - User list rarely changes

### Medium-Lived Data (1 minute TTL)
- `/people` - Contacts change occasionally
- `/deals` - Deals change moderately
- `/tasks` - Tasks change moderately

### Short-Lived Data (30 seconds TTL)
- `/calls` - Calls are frequently added
- `/events` - Events are frequently added
- `/notes` - Notes are frequently added
- `/appointments` - Appointments change frequently

### Default (1 minute TTL)
- Any endpoint not specifically configured

## Performance Impact

### Test Results

**Custom Fields Query** (relatively static data):
- First request (cache miss): **0.381 seconds**
- Second request (cache hit): **0.059 seconds**
- **6.4x faster** with cache
- **Time saved: 0.322 seconds**

### For Bulk Queries

When fetching 1,000 contacts with pagination:
- **Without cache**: 10 API calls √ó ~0.5 seconds = **~5 seconds**
- **With cache** (second time): **~0.05 seconds** (instant!)

## Configuration

Caching is **enabled by default** and can be configured in `src/fub_mcp/config.py`:

```python
# Caching
ENABLE_CACHING: bool = True  # Set to False to disable
CACHE_MAX_SIZE: int = 1000    # Maximum cache entries
```

Or via environment variables:
```bash
ENABLE_CACHING=false  # Disable caching
CACHE_MAX_SIZE=500    # Smaller cache size
```

## Use Cases

### 1. Repeated Dashboard Queries

If an AI client repeatedly asks for:
- List of recent contacts
- Custom fields list
- User list
- Pipeline/stage information

**Result**: First query fetches from API, subsequent queries are instant from cache.

### 2. Bulk Reporting

When generating multiple reports that use the same base data:
```python
# First report - fetches from API
report1 = get_people(limit=1000)

# Second report (different processing) - uses cache!
report2 = get_people(limit=1000)  # Instant!
```

### 3. Frequent Custom Field Lookups

Custom fields are queried often but rarely change:
- Cached for 5 minutes
- Invalidated only on create/update/delete
- Nearly instant responses after first fetch

## Cache Management

### Automatic Management

- **LRU eviction**: When cache is full (1000 entries), oldest entries are removed
- **TTL expiration**: Entries automatically expire based on TTL
- **Write invalidation**: Cache is invalidated on write operations

### Manual Cache Control (Future Enhancement)

Potential future features:
- Cache statistics endpoint
- Manual cache clear
- Per-request cache control (force refresh)

## Best Practices

### ‚úÖ When Caching Helps

1. **Static/reference data**: Custom fields, pipelines, stages, users
2. **Repeated queries**: Same query executed multiple times
3. **Bulk operations**: Large datasets that don't change frequently
4. **Dashboard/reporting**: Multiple reports from same base data

### ‚ö†Ô∏è When to Disable Cache

1. **Real-time requirements**: When you need absolutely fresh data
2. **High-frequency writes**: When data changes very frequently
3. **Testing/debugging**: When you need to see every API call

### üí° Tips

1. **Keep cache enabled** - It's safe and improves performance
2. **Monitor cache stats** - Check hit rates (future feature)
3. **Adjust TTLs** - Customize TTLs in `cache.py` if needed
4. **Clear cache after bulk imports** - Restart server or clear cache manually

## Technical Details

### Cache Implementation

- **Type**: In-memory LRU cache
- **Key**: MD5 hash of `endpoint:params`
- **Storage**: Python `OrderedDict` for O(1) operations
- **Thread-safe**: Uses single cache manager instance

### Memory Usage

- **Estimated**: ~100KB per 1000 cache entries (depends on response size)
- **Max size**: 1000 entries (configurable)
- **Automatic cleanup**: Expired entries are removed on access

### Cache Flow

```
Request ‚Üí Check cache ‚Üí Hit? ‚Üí Return cached data
                    ‚Üì No
                    Make API call ‚Üí Cache response ‚Üí Return data
```

## Summary

‚úÖ **Caching is enabled** and working  
‚úÖ **Automatic invalidation** on writes  
‚úÖ **Smart TTLs** based on data type  
‚úÖ **Significant performance gains** (6x+ faster)  
‚úÖ **Configurable** via environment variables  

**Recommendation**: Keep caching enabled for optimal performance, especially with bulk queries and repeated requests.

---

**Status**: ‚úÖ Implemented and tested  
**Version**: 0.5.0  
**Performance**: 6.4x faster on cached requests


